import streamlit as st
from google import genai
from google.genai import types
import os

# --- CONFIGURACI√ìN DE LA P√ÅGINA ---
st.set_page_config(page_title="Tutor Python IA", page_icon="üêç", layout="centered")

# --- 1. GESTI√ìN DE LA API KEY ---
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
except:
    api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    st.error("‚ö†Ô∏è No se ha encontrado la API Key. Configura 'GOOGLE_API_KEY'.")
    st.stop()

# Cliente Oficial
client = genai.Client(api_key=api_key)

# ==========================================
# üîç ZONA DE DIAGN√ìSTICO (SOLO PARA EL PROFE)
# ==========================================
with st.sidebar:
    st.header("üîß Diagn√≥stico de Modelos")
    st.write("Si da error 404, prueba con uno de estos nombres:")
    
    try:
        # Pedimos a Google la lista de modelos disponibles HOY
        # Iteramos sobre los modelos y filtramos los que generan contenido
        models = client.models.list() 
        valid_models = []
        for m in models:
            # Buscamos modelos que sirvan para 'generateContent'
            # Nota: la estructura del objeto puede variar, imprimimos el nombre directo
            name = m.name.split("/")[-1] # Quitamos el "models/" del principio
            if "gemini" in name and "vision" not in name: # Filtro b√°sico
                valid_models.append(name)
        
        # Mostramos la lista para que puedas copiar
        selected_model = st.selectbox("Modelos detectados:", valid_models, index=0 if valid_models else None)
        st.caption(f"Usando ahora: `{selected_model}`")
        
    except Exception as e:
        st.error(f"No se pudo listar modelos: {e}")
        selected_model = "gemini-1.5-flash" # Fallback por defecto

# ==========================================

# --- 2. CARGA DEL CONTEXTO ---
@st.cache_data 
def load_context():
    try:
        if os.path.exists('contexto.txt'):
            with open('contexto.txt', 'r', encoding='utf-8') as f:
                return f.read()
        return ""
    except: return ""

context_text = load_context()

# --- 3. DEFINICI√ìN DE LA PERSONALIDAD ---
# --- 3. DEFINICI√ìN DE LA PERSONALIDAD (CEREBRO DEL PROFESOR) ---
SYSTEM_PROMPT = f"""
ROL:
Eres el "Tutor IA", un asistente docente experto en Python y pedagog√≠a para alumnos de Secundaria/Bachillerato.
Tu objetivo NO es dar respuestas, sino ense√±ar a pensar.

BASE DE CONOCIMIENTO (CONTEXTO):
Toda tu ense√±anza debe basarse EXCLUSIVAMENTE en el siguiente texto. Si el alumno pregunta algo que no est√° aqu√≠, asume que a√∫n no lo han estudiado.
--------------------------------------------------
{context_text}
--------------------------------------------------

INSTRUCCIONES PARA EL USO DE IM√ÅGENES:
Si en el CONTEXTO anterior aparecen URLs de im√°genes asociadas a un tema, ¬°√öSALAS!
Cuando expliques ese tema, inserta la imagen usando formato Markdown exacto:
![Descripci√≥n breve](URL_DE_LA_IMAGEN)
(Hazlo de forma natural, como: "F√≠jate en este esquema:")

TU ALGORITMO DE RESPUESTA (M√âTODO SOCR√ÅTICO GUIADO):
Cuando el alumno te haga una pregunta o te muestre c√≥digo, sigue estos pasos mentalmente:

1. AN√ÅLISIS: ¬øQu√© intenta hacer el alumno? ¬øQu√© concepto del CONTEXTO necesita usar?
2. DIAGN√ìSTICO: ¬øD√≥nde est√° su error o confusi√≥n?
3. ESTRATEGIA: No le des la soluci√≥n. Divide el problema en el paso m√°s peque√±o posible.
4. ACCI√ìN:
   - Si el c√≥digo tiene error: No lo corrijas. Preg√∫ntale sobre la l√≠nea espec√≠fica. (Ej: "¬øQu√© valor crees que tiene la variable 'x' en la l√≠nea 3?")
   - Si pregunta "¬øC√≥mo se hace X?": P√≠dele que revise una parte concreta de los apuntes o dale una pista de sintaxis incompleta.
   - Si est√° bloqueado: Dale un ejemplo parecida (an√°logo) pero con otros datos, para que √©l deduzca la regla.

REGLAS DE ORO (MANDAMIENTOS):
- JAM√ÅS escribas el c√≥digo completo de la soluci√≥n. NUNCA.
- Si te piden "Hazme el ejercicio", responde: "Yo soy tu copiloto, no el piloto. Escribe t√∫ c√≥mo empezar√≠as y yo te corrijo".
- S√© paciente, amable y usa emojis ocasionalmente (üêç, üíª, üí°).
- Si el concepto implica una imagen del contexto, mu√©strala.
- PREGUNTAS GU√çA: Termina tus intervenciones con una pregunta sencilla que les obligue a deducir el siguiente paso.

EJEMPLOS DE INTERACCI√ìN DESEADA:

Alumno: "No me funciona el bucle."
Tutor (MAL): "Te falta poner dos puntos al final de la l√≠nea while."
Tutor (BIEN): "¬°Casi lo tienes! Mira bien la l√≠nea del 'while'. En Python, ¬øqu√© signo de puntuaci√≥n necesitamos poner siempre al final de una instrucci√≥n de bloque (como if o for) para decir 'aqu√≠ empieza lo de dentro'? üßê"

Alumno: "¬øC√≥mo sumo dos variables?"
Tutor (BIEN): "Para sumar usamos un operador matem√°tico, igual que en clase de mates. Si tienes 'a' y 'b', ¬øc√≥mo lo escribir√≠as en papel? Intenta escribir el c√≥digo t√∫ mismo aqu√≠."

"""

# --- 4. GESTI√ìN DE LA SESI√ìN ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Configuraci√≥n del chat usando el modelo seleccionado en la barra lateral
if "chat_session" not in st.session_state or st.session_state.current_model != selected_model:
    st.session_state.current_model = selected_model
    try:
        st.session_state.chat_session = client.chats.create(
            model=selected_model, 
            config=types.GenerateContentConfig(
                system_instruction=SYSTEM_PROMPT,
                temperature=0.7
            )
        )
    except Exception as e:
        st.error(f"Error al iniciar chat con {selected_model}: {e}")

# --- 5. INTERFAZ GR√ÅFICA ---
st.title("üêç Tutor de Python")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- 6. INTERACCI√ìN ---
if prompt := st.chat_input("Escribe tu duda..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    try:
        with st.spinner(f"Pensando con {selected_model}..."):
            response = st.session_state.chat_session.send_message(prompt)
            bot_reply = response.text
            
        with st.chat_message("assistant"):
            st.markdown(bot_reply)
        st.session_state.messages.append({"role": "assistant", "content": bot_reply})
        
    except Exception as e:
        st.error(f"Error de conexi√≥n (Intenta cambiar el modelo en la barra izquierda): {e}")


